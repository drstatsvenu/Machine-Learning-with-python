{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools or some other automated techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data for the first time, you should ask yourself a few questions:\n",
    "- Do I need all of the variables?\n",
    "- Should I transform any variables?\n",
    "- Are there NA values, outliers or other strange values?\n",
    "- Should I create new variables?\n",
    "\n",
    "\n",
    "For the rest of this lesson we will address each of these questions in the context of this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Do I need all the variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of unnecessary variables is a good first step when dealing with any data set, since dropping variables reduces complexity and can make computation on the data faster. Whether you should get rid of a variable or not will depend on size of the data set and the goal of your analysis. With a data set as small as the Titanic data, there's no real need to drop variables from a computing perspective (we have plenty of memory and processing power to deal with such a small data set) but it can still be helpful to drop variables that will only distract from your goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each variable and consider whether we should keep it or not in the context of predicting survival:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"PassengerId\" is just a number assigned to each passenger. It is nothing more than an arbitrary identifier; we could keep it for identification purposes, but let's remove it anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# deleting passengerid column\n",
    "del train['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Survived\" indicates whether each passenger lived or died. Since predicting survival is our goal, we definitely need to keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that describe passengers numerically or group them into a few broad categories could be useful for predicting survival. The variables Pclass, Sex, Age, SibSp, Parch, Fare and Embarked appear to fit this description, so let's keep all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 more features to consider: Name, Ticket and Cabin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Name\" appears to be a character string of the name of each passenger. Let's look at name a little closer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abbing, Mr. Anthony',\n",
       " 'Abbott, Mr. Rossmore Edward',\n",
       " 'Abbott, Mrs. Stanton (Rosa Hunt)',\n",
       " 'Abelson, Mr. Samuel',\n",
       " 'Abelson, Mrs. Samuel (Hannah Wizosky)',\n",
       " 'Adahl, Mr. Mauritz Nils Martin',\n",
       " 'Adams, Mr. John',\n",
       " 'Ahlin, Mrs. Johan (Johanna Persdotter Larsson)',\n",
       " 'Aks, Mrs. Sam (Leah Rosen)',\n",
       " 'Albimona, Mr. Nassef Cassem',\n",
       " 'Alexander, Mr. William',\n",
       " 'Alhomaki, Mr. Ilmari Rudolf',\n",
       " 'Ali, Mr. Ahmed',\n",
       " 'Ali, Mr. William',\n",
       " 'Allen, Miss. Elisabeth Walton']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(train['Name'])[0:15]\n",
    "\n",
    "# we are sorting the names in the alphabetical order and printing the first 15 names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                         891\n",
       "unique                        891\n",
       "top       Braund, Mr. Owen Harris\n",
       "freq                            1\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Name'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we see that the Name variable has 891 unique values. Since there are 891 rows in the data set we know each name is unique. It appears that married women have their maiden names listed in parentheses. In general, a categorical variable that is unique to each case isn't useful for prediction. We could extract last names to try to group family members together, but even then the number of categories would be very large. In addition, the Parch and SibSp variables already contain some information about family relationships, so from the perspective of predictive modeling, the Name variable could be removed. But let us keep it with us knowing that that we will not be using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a closer look at tickets column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            A/5 21171\n",
       "1             PC 17599\n",
       "2     STON/O2. 3101282\n",
       "3               113803\n",
       "4               373450\n",
       "5               330877\n",
       "6                17463\n",
       "7               349909\n",
       "8               347742\n",
       "9               237736\n",
       "10             PP 9549\n",
       "11              113783\n",
       "12           A/5. 2151\n",
       "13              347082\n",
       "14              350406\n",
       "Name: Ticket, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Ticket'][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        891\n",
       "unique       681\n",
       "top       347082\n",
       "freq           7\n",
       "Name: Ticket, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Ticket'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables with almost as many levels as there are records are generally not very useful for prediction. Ticket has 681 unique values: almost as many as there are passengers. We could try to reduce the number of levels by grouping certain tickets together, but the ticket numbers don't appear to follow any logical pattern we could use for grouping. Let's remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train['Ticket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally cabin variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      C85\n",
       "2      NaN\n",
       "3     C123\n",
       "4      NaN\n",
       "5      NaN\n",
       "6      E46\n",
       "7      NaN\n",
       "8      NaN\n",
       "9      NaN\n",
       "10      G6\n",
       "11    C103\n",
       "12     NaN\n",
       "13     NaN\n",
       "14     NaN\n",
       "Name: Cabin, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Cabin'][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count             204\n",
       "unique            147\n",
       "top       C23 C25 C27\n",
       "freq                4\n",
       "Name: Cabin, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Cabin'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin also has 147 unique values, which indicates it may not be particularly useful for prediction. On the other hand, the names of the levels for the cabin variable seem to have a regular structure: each starts with a capital letter followed by a number. We could use that structure to reduce the number of levels to make categories large enough that they might be useful for prediction. Let's Keep Cabin for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, removing variables is often more of an art than a science. It is easiest to start simple: don't be afraid to remove (or simply ignore) confusing, messy or otherwise troublesome variables temporarily when you're just getting starting with an analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data projects are iterative processes: you can start with a simple analysis or model using only a few variables and then expand later by adding more and more of the other variables you initially ignored or removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
