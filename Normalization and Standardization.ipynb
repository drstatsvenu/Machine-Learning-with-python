{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples we have looked at how feature scaling works.\n",
    "\n",
    "Let us now look at two particular methods Normalization and Standardization with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization refers to rescaling real valued numeric attributes into the range 0 and 1.\n",
    "\n",
    "It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression.\n",
    "\n",
    "The example below demonstrate data normalization of the Iris flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data attributes for the Iris dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "iris.data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data from the target attributes\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# normalize the data attributes\n",
    "\n",
    "normalized_X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80377277,  0.55160877,  0.22064351,  0.0315205 ],\n",
       "       [ 0.82813287,  0.50702013,  0.23660939,  0.03380134],\n",
       "       [ 0.80533308,  0.54831188,  0.2227517 ,  0.03426949],\n",
       "       [ 0.80003025,  0.53915082,  0.26087943,  0.03478392],\n",
       "       [ 0.790965  ,  0.5694948 ,  0.2214702 ,  0.0316386 ],\n",
       "       [ 0.78417499,  0.5663486 ,  0.2468699 ,  0.05808704],\n",
       "       [ 0.78010936,  0.57660257,  0.23742459,  0.0508767 ],\n",
       "       [ 0.80218492,  0.54548574,  0.24065548,  0.0320874 ],\n",
       "       [ 0.80642366,  0.5315065 ,  0.25658935,  0.03665562],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data looks after mean normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance).\n",
    "\n",
    "It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes.\n",
    "\n",
    "The example below demonstrate data standardization of the Iris flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the data attributes for the Iris dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the Iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "iris.data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data and target attributes\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# standardize the data attributes\n",
    "\n",
    "standardized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.03205722, -1.3412724 , -1.31297673],\n",
       "       [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673],\n",
       "       [-1.38535265,  0.33784833, -1.39813811, -1.31297673],\n",
       "       [-1.50652052,  0.10644536, -1.2844067 , -1.31297673],\n",
       "       [-1.02184904,  1.26346019, -1.3412724 , -1.31297673],\n",
       "       [-0.53717756,  1.95766909, -1.17067529, -1.05003079],\n",
       "       [-1.50652052,  0.80065426, -1.3412724 , -1.18150376],\n",
       "       [-1.02184904,  0.80065426, -1.2844067 , -1.31297673],\n",
       "       [-1.74885626, -0.35636057, -1.3412724 , -1.31297673],\n",
       "       [-1.14301691,  0.10644536, -1.2844067 , -1.4444497 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is the standardised data for the original iris data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to know whether rescaling your data will improve the performance of your algorithms before you apply them. If often can, but not always."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
